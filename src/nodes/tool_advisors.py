"""9. ToolAdvisors ë…¸ë“œ - ë„êµ¬ ì¡°ì–¸ ëª¨ë“ˆ (ì„ íƒì )"""
from typing import Dict, Any, List
from ..state import GraphState


def tool_advisors_node(state: GraphState) -> Dict[str, Any]:
    """
    ì„¤ì •/ëª¨ë¸/ì¬ë­ì»¤/í‰ê°€ í…œí”Œë¦¿ ì¶”ì²œ
    
    ì…ë ¥: plan, user.constraints
    ì¶œë ¥: í›„ë³´ ì˜µì…˜ í‘œ (ì¥ë‹¨ì /ë¹„ìš©/ì§€ì—°)
    """
    print("ğŸ› ï¸ [ToolAdvisors] ë„êµ¬ ì¶”ì²œ ìƒì„± ì¤‘...")
    
    user = state["user"]
    constraints = user.constraints
    experience_level = user.prefs.get("experience_level", 1)
    
    tool_advice = []
    
    # ì„ë² ë”© ëª¨ë¸ ì¶”ì²œ
    embedding_options = [
        {
            "category": "embedding",
            "name": "OpenAI text-embedding-3-small",
            "pros": ["ë¹ ë¥¸ ì†ë„", "ë‚®ì€ ë¹„ìš©", "ê°„ë‹¨í•œ API"],
            "cons": ["í•œêµ­ì–´ ì„±ëŠ¥ ë³´í†µ", "ì™¸ë¶€ ì˜ì¡´ì„±"],
            "cost": "ë‚®ìŒ ($0.02/1M tokens)",
            "latency": "ë‚®ìŒ (~100ms)",
            "suitable_for": [0, 1]
        },
        {
            "category": "embedding",
            "name": "multilingual-e5-large",
            "pros": ["ìš°ìˆ˜í•œ í•œêµ­ì–´ ì„±ëŠ¥", "ì˜¤í”ˆì†ŒìŠ¤", "ìì²´ í˜¸ìŠ¤íŒ… ê°€ëŠ¥"],
            "cons": ["í° ëª¨ë¸ í¬ê¸°", "GPU í•„ìš”"],
            "cost": "ì¤‘ê°„ (í˜¸ìŠ¤íŒ… ë¹„ìš©)",
            "latency": "ì¤‘ê°„ (~200ms)",
            "suitable_for": [1, 2, 3]
        },
        {
            "category": "embedding",
            "name": "bge-m3",
            "pros": ["ë‹¤ì¤‘ ì–¸ì–´ ì§€ì›", "dense+sparse í•˜ì´ë¸Œë¦¬ë“œ", "SOTA ì„±ëŠ¥"],
            "cons": ["ë†’ì€ ì»´í“¨íŒ… ìš”êµ¬", "ë³µì¡í•œ ì„¤ì •"],
            "cost": "ë†’ìŒ",
            "latency": "ë†’ìŒ (~300ms)",
            "suitable_for": [2, 3]
        }
    ]
    
    # ê²½í—˜ ìˆ˜ì¤€ì— ë§ëŠ” ì„ë² ë”© ì¶”ì²œ
    for option in embedding_options:
        if experience_level in option["suitable_for"]:
            tool_advice.append(option)
    
    # ì¬ë­í‚¹ ì˜µì…˜
    if experience_level >= 1:
        rerank_options = [
            {
                "category": "reranking",
                "name": "cross-encoder (ms-marco)",
                "pros": ["ë†’ì€ ì •í™•ë„", "ê²€ì¦ëœ ëª¨ë¸"],
                "cons": ["ì¶”ê°€ ì§€ì—° (~100ms/doc)", "GPU ê¶Œì¥"],
                "cost": "ì¤‘ê°„",
                "latency": "ì¤‘ê°„",
                "when_to_use": "ì •í™•ë„ê°€ ì¤‘ìš”í•˜ê³  ë¬¸ì„œê°€ 10ê°œ ì´í•˜ì¼ ë•Œ"
            },
            {
                "category": "reranking",
                "name": "LLM-based reranking",
                "pros": ["ìµœê³  ì •í™•ë„", "ì»¨í…ìŠ¤íŠ¸ ì´í•´"],
                "cons": ["ë†’ì€ ë¹„ìš©", "ë†’ì€ ì§€ì—°"],
                "cost": "ë†’ìŒ",
                "latency": "ë†’ìŒ",
                "when_to_use": "ê³ ê°€ì¹˜ ì¿¼ë¦¬, ë³µì¡í•œ íŒë‹¨ í•„ìš”ì‹œ"
            }
        ]
        
        if "quality_priority" in constraints:
            tool_advice.extend(rerank_options)
    
    # í‰ê°€ ë„êµ¬
    if experience_level >= 1:
        eval_option = {
            "category": "evaluation",
            "name": "RAGAS",
            "pros": ["í¬ê´„ì  ì§€í‘œ", "ìë™í™” ê°€ëŠ¥", "ì˜¤í”ˆì†ŒìŠ¤"],
            "cons": ["LLM API ë¹„ìš©", "ì„¤ì • í•„ìš”"],
            "metrics": ["Faithfulness", "Answer Relevance", "Context Precision", "Context Recall"],
            "setup_difficulty": "ì¤‘ê°„"
        }
        tool_advice.append(eval_option)
    
    print(f"âœ… [ToolAdvisors] ì¶”ì²œ ì™„ë£Œ: {len(tool_advice)}ê°œ ë„êµ¬")
    
    return {
        "tool_advice": tool_advice,
        "current_step": "tool_advisors"
    }

